{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "__Code Citation__: Fast Yet Effective Machine Unlearning, Ayush K Tarun, Vikram S Chundawat, Murari Mandal, Mohan Kankanhalli,\n",
    " https://github.com/vikram2000b/Fast-Machine-Unlearning/blob/main/Machine%20Unlearning.ipynb, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import os, sys\n",
    "import time\n",
    "import pickle\n",
    "from sklearn import datasets as sklearn_dataset\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "sys.path.append('../')\n",
    "from Unmunge_Machine_Unlearning.utils_unlearn import *\n",
    "\n",
    "torch.manual_seed(100)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def training_step(model, batch):\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    out = model(images)                  \n",
    "    loss = F.cross_entropy(out, labels) \n",
    "    return loss\n",
    "\n",
    "def validation_step(model, batch):\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    out = model(images)                    \n",
    "    loss = F.cross_entropy(out, labels)   \n",
    "    acc = accuracy(out, labels)\n",
    "    return {'Loss': loss.detach(), 'Acc': acc}\n",
    "\n",
    "def validation_epoch_end(model, outputs):\n",
    "    batch_losses = [x['Loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()   \n",
    "    batch_accs = [x['Acc'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()      \n",
    "    return {'Loss': epoch_loss.item(), 'Acc': epoch_acc.item()}\n",
    "\n",
    "def epoch_end(model, epoch, result):\n",
    "    print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "        epoch, result['lrs'][-1], result['train_loss'], result['Loss'], result['Acc']))\n",
    "    \n",
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [validation_step(model, batch) for batch in val_loader]\n",
    "    return validation_epoch_end(model, outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = training_step(model, batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            lrs.append(get_lr(optimizer))\n",
    "            \n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        epoch_end(model, epoch, result)\n",
    "        history.append(result)\n",
    "        sched.step(result['Loss'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "def load_casia_webface(root='./'):\n",
    "    \n",
    "    with gzip.open(root, 'rb') as f:\n",
    "        train_ds, test_ds = pickle.load(f)\n",
    "    \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/rajdeep/workspace/Datasets/CASIA-WebFace/casia-webface-dataset.pkl.gz'\n",
    "train_ds, valid_ds = load_casia_webface(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  128\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "# model = resnet18(num_classes = 40, ).to(device = device)\n",
    "model = resnet18(num_classes=300)\n",
    "# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model = model.to(device)\n",
    "\n",
    "epochs = 40\n",
    "max_lr = 0.01\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam\n",
    "model_path = '/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "\n",
      "Loading pre-trained network checkpoint from: \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "\n",
      "Loaded pre-trained network checkpoint from \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "epoch: 46 train loss: 0.012277959337188851 test loss: 1.3651225207603142\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajdeep/workspace/Machine_Unlearning/Fast-Machine-Unlearning-main_modified/../Unmunge_Machine_Unlearning/utils_unlearn.py:916: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(network_path, map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "obj_model = utils_(image_size=(64, 64),\n",
    "                            num_input_channels = 3,\n",
    "                            num_classes = 300,\n",
    "                            learning_rate = 1e-3,\n",
    "                            batch_size = 128,\n",
    "                            num_epochs = 30,\n",
    "                            padding = True,\n",
    "                            model_save_name = '',\n",
    "                            data_name = 'casia-webface',\n",
    "                            model_name='ResNet18',\n",
    "                            unlearn_cls = '',\n",
    "                            solver_type = 'adam',\n",
    "                            result_savepath = './comaprison_results',\n",
    "                            retrained_models_folder_name = '',\n",
    "                            unlearned_models_folder_name = '',\n",
    "                            unlearn_type=''\n",
    "                                )\n",
    "\n",
    "obj_model.load_network(model_path)\n",
    "model = obj_model.network.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Loss': 1.4367653131484985, 'Acc': 0.8096902370452881}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [evaluate(model, valid_dl)]\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the noise structure\n",
    "class Noise(nn.Module):\n",
    "    def __init__(self, *dim):\n",
    "        super().__init__()\n",
    "        self.noise = torch.nn.Parameter(torch.randn(*dim), requires_grad = True)\n",
    "        \n",
    "    def forward(self):\n",
    "        return self.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all classes\n",
    "classes = list(range(300))#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# classes which are required to un-learn\n",
    "classes_to_forget = [7]#[0, 2, 7, 9, 31, 30, 33, 32]#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classwise list of samples\n",
    "num_classes = 300\n",
    "classwise_train = {}\n",
    "for i in range(num_classes):\n",
    "    classwise_train[i] = []\n",
    "\n",
    "for img, label in train_ds:\n",
    "    classwise_train[label].append((img, label))\n",
    "    \n",
    "classwise_test = {}\n",
    "for i in range(num_classes):\n",
    "    classwise_test[i] = []\n",
    "\n",
    "for img, label in valid_ds:\n",
    "    classwise_test[label].append((img, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting some samples from retain classes\n",
    "num_samples_per_class = 1000\n",
    "\n",
    "retain_samples = []\n",
    "for i in range(len(classes)):\n",
    "    if classes[i] not in classes_to_forget:\n",
    "        retain_samples += classwise_train[i][:num_samples_per_class]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain validation set\n",
    "retain_valid = []\n",
    "for cls in range(num_classes):\n",
    "    if cls not in classes_to_forget:\n",
    "        for img, label in classwise_test[cls]:\n",
    "            retain_valid.append((img, label))\n",
    "            \n",
    "# forget validation set\n",
    "forget_valid = []\n",
    "for cls in range(num_classes):\n",
    "    if cls in classes_to_forget:\n",
    "        for img, label in classwise_test[cls]:\n",
    "            forget_valid.append((img, label))\n",
    "            \n",
    "forget_valid_dl = DataLoader(forget_valid, batch_size, num_workers=3, pin_memory=True)\n",
    "retain_valid_dl = DataLoader(retain_valid, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19126"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retain_valid_dl.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "\n",
      "Loading pre-trained network checkpoint from: \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "\n",
      "Loaded pre-trained network checkpoint from \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "epoch: 46 train loss: 0.012277959337188851 test loss: 1.3651225207603142\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "obj_model = utils_(image_size=(64, 64),\n",
    "                            num_input_channels = 3,\n",
    "                            num_classes = 300,\n",
    "                            learning_rate = 1e-3,\n",
    "                            batch_size = 128,\n",
    "                            num_epochs = 30,\n",
    "                            padding = True,\n",
    "                            model_save_name = '',\n",
    "                            data_name = 'casia-webface',\n",
    "                            model_name='ResNet18',\n",
    "                            unlearn_cls = '',\n",
    "                            solver_type = 'adam',\n",
    "                            result_savepath = './comaprison_results',\n",
    "                            retrained_models_folder_name = '',\n",
    "                            unlearned_models_folder_name = '',\n",
    "                            unlearn_type=''\n",
    "                                )\n",
    "\n",
    "obj_model.load_network(model_path)\n",
    "model = obj_model.network.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes to unlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_forget_list = [[252], [162], [2], [150], [188], [156], [94], [191], [292], [169]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unearning using UNSIR algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "\n",
      "Loading pre-trained network checkpoint from: \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "\n",
      "Loaded pre-trained network checkpoint from \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "epoch: 46 train loss: 0.012277959337188851 test loss: 1.3651225207603142\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "<timed exec>:150: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 35.51320266723633\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 69.64457631111145\n",
      "Loss: 1.3248932361602783\n",
      "\n",
      "Model Architecture:\n",
      "\n",
      "Loading pre-trained network checkpoint from: \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "\n",
      "Loaded pre-trained network checkpoint from \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "epoch: 46 train loss: 0.012277959337188851 test loss: 1.3651225207603142\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 29.144210815429688\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 71.8606173992157\n",
      "Loss: 1.2507598400115967\n",
      "\n",
      "Model Architecture:\n",
      "\n",
      "Loading pre-trained network checkpoint from: \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "\n",
      "Loaded pre-trained network checkpoint from \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "epoch: 46 train loss: 0.012277959337188851 test loss: 1.3651225207603142\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 28.964794158935547\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 69.6699321269989\n",
      "Loss: 1.379740834236145\n",
      "\n",
      "Model Architecture:\n",
      "\n",
      "Loading pre-trained network checkpoint from: \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "\n",
      "Loaded pre-trained network checkpoint from \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "epoch: 46 train loss: 0.012277959337188851 test loss: 1.3651225207603142\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 25.284019470214844\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 68.57947111129761\n",
      "Loss: 1.43768310546875\n",
      "\n",
      "Model Architecture:\n",
      "\n",
      "Loading pre-trained network checkpoint from: \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "\n",
      "Loaded pre-trained network checkpoint from \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "epoch: 46 train loss: 0.012277959337188851 test loss: 1.3651225207603142\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 34.947364807128906\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 62.88567781448364\n",
      "Loss: 1.6201608180999756\n",
      "\n",
      "Model Architecture:\n",
      "\n",
      "Loading pre-trained network checkpoint from: \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "\n",
      "Loaded pre-trained network checkpoint from \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "epoch: 46 train loss: 0.012277959337188851 test loss: 1.3651225207603142\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 28.444530487060547\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 66.45504236221313\n",
      "Loss: 1.500975251197815\n",
      "\n",
      "Model Architecture:\n",
      "\n",
      "Loading pre-trained network checkpoint from: \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "\n",
      "Loaded pre-trained network checkpoint from \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "epoch: 46 train loss: 0.012277959337188851 test loss: 1.3651225207603142\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 31.544361114501953\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 69.23806071281433\n",
      "Loss: 1.3689076900482178\n",
      "\n",
      "Model Architecture:\n",
      "\n",
      "Loading pre-trained network checkpoint from: \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "\n",
      "Loaded pre-trained network checkpoint from \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "epoch: 46 train loss: 0.012277959337188851 test loss: 1.3651225207603142\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 32.053131103515625\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 69.55820322036743\n",
      "Loss: 1.3278250694274902\n",
      "\n",
      "Model Architecture:\n",
      "\n",
      "Loading pre-trained network checkpoint from: \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "\n",
      "Loaded pre-trained network checkpoint from \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "epoch: 46 train loss: 0.012277959337188851 test loss: 1.3651225207603142\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 36.53543472290039\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 70.72010636329651\n",
      "Loss: 1.2889783382415771\n",
      "\n",
      "Model Architecture:\n",
      "\n",
      "Loading pre-trained network checkpoint from: \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "\n",
      "Loaded pre-trained network checkpoint from \"/home/rajdeep/workspace/Machine_Unlearning/Unmunge_Machine_Unlearning/results/casia-webface/ResNet18/ResNet18_casia-webface_best_network.pth\"\n",
      "epoch: 46 train loss: 0.012277959337188851 test loss: 1.3651225207603142\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 22.481332778930664\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 71.65492177009583\n",
      "Loss: 1.2864421606063843\n",
      "CPU times: user 1min 39s, sys: 7.08 s, total: 1min 47s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for classes_to_forget in classes_to_forget_list:\n",
    "    \n",
    "    # classwise list of samples\n",
    "    num_classes = 300\n",
    "    classwise_train = {}\n",
    "    for i in range(num_classes):\n",
    "        classwise_train[i] = []\n",
    "\n",
    "    for img, label in train_ds:\n",
    "        classwise_train[label].append((img, label))\n",
    "        \n",
    "    classwise_test = {}\n",
    "    for i in range(num_classes):\n",
    "        classwise_test[i] = []\n",
    "\n",
    "    for img, label in valid_ds:\n",
    "        classwise_test[label].append((img, label))\n",
    "        \n",
    "        \n",
    "        # getting some samples from retain classes\n",
    "    num_samples_per_class = 1000\n",
    "\n",
    "    retain_samples = []\n",
    "    for i in range(len(classes)):\n",
    "        if classes[i] not in classes_to_forget:\n",
    "            retain_samples += classwise_train[i][:num_samples_per_class]\n",
    "        \n",
    "        \n",
    "    # retain validation set\n",
    "    retain_valid = []\n",
    "    for cls in range(num_classes):\n",
    "        if cls not in classes_to_forget:\n",
    "            for img, label in classwise_test[cls]:\n",
    "                retain_valid.append((img, label))\n",
    "                \n",
    "    # forget validation set\n",
    "    forget_valid = []\n",
    "    for cls in range(num_classes):\n",
    "        if cls in classes_to_forget:\n",
    "            for img, label in classwise_test[cls]:\n",
    "                forget_valid.append((img, label))\n",
    "                \n",
    "    forget_valid_dl = DataLoader(forget_valid, batch_size, num_workers=3, pin_memory=True)\n",
    "    retain_valid_dl = DataLoader(retain_valid, batch_size*2, num_workers=3, pin_memory=True)\n",
    "\n",
    "    obj_model = utils_(image_size=(64, 64),\n",
    "                                num_input_channels = 3,\n",
    "                                num_classes = 300,\n",
    "                                learning_rate = 1e-3,\n",
    "                                batch_size = 128,\n",
    "                                num_epochs = 30,\n",
    "                                padding = True,\n",
    "                                model_save_name = '',\n",
    "                                data_name = 'casia-webface',\n",
    "                                model_name='ResNet18',\n",
    "                                unlearn_cls = '',\n",
    "                                solver_type = 'adam',\n",
    "                                result_savepath = './comaprison_results',\n",
    "                                retrained_models_folder_name = '',\n",
    "                                unlearned_models_folder_name = '',\n",
    "                                unlearn_type=''\n",
    "                                    )\n",
    "\n",
    "    obj_model.load_network(model_path)\n",
    "    model = obj_model.network.to(device)\n",
    "\n",
    "    ## Noise Generation using UNSIR ##\n",
    "    batch_size = 256\n",
    "    noises = {}\n",
    "    time_list = []\n",
    "    for cls in classes_to_forget:\n",
    "        time1 = time.time()\n",
    "        # print(\"Optiming loss for class {}\".format(cls))\n",
    "        size = (batch_size, ) + tuple(train_ds[0][0].shape)\n",
    "        noises[cls] = Noise(*size).to(device)\n",
    "        # noises[cls] = Noise(batch_size, 3, 32, 32).to(device)\n",
    "        opt = torch.optim.Adam(noises[cls].parameters(), lr = 0.1)\n",
    "\n",
    "        num_epochs = 5\n",
    "        num_steps = 8\n",
    "        class_label = cls\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = []\n",
    "            for batch in range(num_steps):\n",
    "                inputs = noises[cls]()\n",
    "                labels = torch.zeros(batch_size).to(device)+class_label\n",
    "                outputs = model(inputs)\n",
    "                loss = -F.cross_entropy(outputs, labels.long()) + 0.1*torch.mean(torch.sum(torch.square(inputs), [1, 2, 3]))\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total_loss.append(loss.cpu().detach().numpy())\n",
    "            # print(\"Loss: {}\".format(np.mean(total_loss)))\n",
    "        time2 = time.time()\n",
    "        req_time = time2-time1\n",
    "        time_list.append(req_time)\n",
    "        \n",
    "    \n",
    "    ## Impair Step ##    \n",
    "    batch_size = 256\n",
    "    noisy_data = []\n",
    "    num_batches = 20\n",
    "    class_num = 0\n",
    "\n",
    "    for cls in classes_to_forget:\n",
    "        for i in range(num_batches):\n",
    "            batch = noises[cls]().cpu().detach()\n",
    "            for i in range(batch[0].size(0)):\n",
    "                noisy_data.append((batch[i], torch.tensor(class_num)))\n",
    "\n",
    "    other_samples = []\n",
    "    for i in range(len(retain_samples)):\n",
    "        other_samples.append((retain_samples[i][0].cpu(), torch.tensor(retain_samples[i][1])))\n",
    "    noisy_data += other_samples\n",
    "    noisy_loader = torch.utils.data.DataLoader(noisy_data, batch_size=256, shuffle = True)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.02)\n",
    "    for epoch in range(1):  \n",
    "        model.train(True)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0\n",
    "        for i, data in enumerate(noisy_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device),torch.tensor(labels).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            out = torch.argmax(outputs.detach(),dim=1)\n",
    "            assert out.shape==labels.shape\n",
    "            running_acc += (labels==out).sum().item()\n",
    "        # print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)},Train Acc:{running_acc*100/len(train_ds)}%\")\n",
    "        \n",
    "        \n",
    "    ## Repair Step ##    \n",
    "    heal_loader = torch.utils.data.DataLoader(other_samples, batch_size=256, shuffle = True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "    for epoch in range(1):  \n",
    "        model.train(True)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0\n",
    "        for i, data in enumerate(heal_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device),torch.tensor(labels).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            out = torch.argmax(outputs.detach(),dim=1)\n",
    "            assert out.shape==labels.shape\n",
    "            running_acc += (labels==out).sum().item()\n",
    "            \n",
    "        # print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)},Train Acc:{running_acc*100/len(train_ds)}%\")\n",
    "    \n",
    "    print(\"Performance of Standard Forget Model on Forget Class\")\n",
    "    history1 = [evaluate(model, forget_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history1[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history1[0][\"Loss\"]))\n",
    "\n",
    "    print(\"Performance of Standard Forget Model on Retain Class\")\n",
    "    history2 = [evaluate(model, retain_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history2[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history2[0][\"Loss\"]))\n",
    "\n",
    "    results.append({'class':classes_to_forget, 'unlearn_accuracy': history1[0][\"Acc\"]*100, 'retain_accuracy': history2[0][\"Acc\"]*100})      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>unlearn_accuracy</th>\n",
       "      <th>retain_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[252]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.644576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[162]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.860617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.669932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[150]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.579471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[188]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.885678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[156]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.455042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[94]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.238061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[191]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.558203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[292]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.720106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[169]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.654922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  unlearn_accuracy  retain_accuracy\n",
       "0  [252]               0.0        69.644576\n",
       "1  [162]               0.0        71.860617\n",
       "2    [2]               0.0        69.669932\n",
       "3  [150]               0.0        68.579471\n",
       "4  [188]               0.0        62.885678\n",
       "5  [156]               0.0        66.455042\n",
       "6   [94]               0.0        69.238061\n",
       "7  [191]               0.0        69.558203\n",
       "8  [292]               0.0        70.720106\n",
       "9  [169]               0.0        71.654922"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# time_list\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unlearn_accuracy</th>\n",
       "      <th>retain_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>69.026661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.655279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>62.885678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>68.744119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>69.601390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>70.457563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>71.860617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unlearn_accuracy  retain_accuracy\n",
       "count              10.0        10.000000\n",
       "mean                0.0        69.026661\n",
       "std                 0.0         2.655279\n",
       "min                 0.0        62.885678\n",
       "25%                 0.0        68.744119\n",
       "50%                 0.0        69.601390\n",
       "75%                 0.0        70.457563\n",
       "max                 0.0        71.860617"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condapy312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
